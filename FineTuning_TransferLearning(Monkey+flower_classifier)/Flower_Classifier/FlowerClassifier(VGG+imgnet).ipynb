{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13478219-3a39-492d-9f4c-8e593ada0ab0",
   "metadata": {},
   "source": [
    "## Making a Flower Classifier with VGG16\n",
    "#### Loading the VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5c8c6e4-b2e4-49a5-8e87-69ec3a940e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 8s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "# VGG16 was designed to work on 224x224 pixel input images sizes\n",
    "\n",
    "img_rows = 224\n",
    "img_cols = 224\n",
    "\n",
    "#Load the VGG16 model\n",
    "vgg16 = VGG16(weights =  'imagenet',\n",
    "              include_top = False, #we are not adding top of the file as they are pretrained\n",
    "             input_shape = (img_rows,img_cols,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df83085-0c10-49f6-8e45-f0eb4a7e88b1",
   "metadata": {},
   "source": [
    "## Inspecting each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47fb2de2-5c91-4710-aaff-273c5ac151cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 InputLayer True\n",
      "1 Conv2D True\n",
      "2 Conv2D True\n",
      "3 MaxPooling2D True\n",
      "4 Conv2D True\n",
      "5 Conv2D True\n",
      "6 MaxPooling2D True\n",
      "7 Conv2D True\n",
      "8 Conv2D True\n",
      "9 Conv2D True\n",
      "10 MaxPooling2D True\n",
      "11 Conv2D True\n",
      "12 Conv2D True\n",
      "13 Conv2D True\n",
      "14 MaxPooling2D True\n",
      "15 Conv2D True\n",
      "16 Conv2D True\n",
      "17 Conv2D True\n",
      "18 MaxPooling2D True\n"
     ]
    }
   ],
   "source": [
    "## Let's print our layers\n",
    "for (i,layer) in enumerate(vgg16.layers):\n",
    "     print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac8385c-31b6-4926-91b3-8fdb809e62c0",
   "metadata": {},
   "source": [
    "## Let's Freeze all the top layer except 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9469f513-d599-457c-9d4a-e447c47a51c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 InputLayer False\n",
      "1 Conv2D False\n",
      "2 Conv2D False\n",
      "3 MaxPooling2D False\n",
      "4 Conv2D False\n",
      "5 Conv2D False\n",
      "6 MaxPooling2D False\n",
      "7 Conv2D False\n",
      "8 Conv2D False\n",
      "9 Conv2D False\n",
      "10 MaxPooling2D False\n",
      "11 Conv2D False\n",
      "12 Conv2D False\n",
      "13 Conv2D False\n",
      "14 MaxPooling2D False\n",
      "15 Conv2D False\n",
      "16 Conv2D False\n",
      "17 Conv2D False\n",
      "18 MaxPooling2D False\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "#Vgg16 was designed to work on 244x244 pixel input image sizes\n",
    "img_rows= 224\n",
    "img_cols = 224\n",
    "\n",
    "#Re-load the vgg16 model without the top or fc layers\n",
    "vgg16 = VGG16(weights='imagenet',\n",
    "             include_top=False,\n",
    "             input_shape = (img_rows,img_cols,3))\n",
    "\n",
    "# include_top=True includes extra 4 layer of \n",
    "# 19 Flatten False\n",
    "# 20 Dense False\n",
    "# 21 Dense False\n",
    "# 22 Dense False\n",
    "\n",
    "# Here we freeze the last 4 layers \n",
    "# Layers are set to trainable as True by default\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Let's print our layers \n",
    "for (i,layer) in enumerate(vgg16.layers):\n",
    "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)\n",
    "    \n",
    "#Now all the layers have been set as false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b17500f-2d6a-4b25-92b2-7d62a04ee2d3",
   "metadata": {},
   "source": [
    "## Making a function that returns our FC Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "86160bd8-70b5-42c0-a232-b47364b0bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTopModel(bottom_model,num_classes,D=256): #Here D = dense layer size\n",
    "    \"\"\" creates the top or head of the model that will be \n",
    "    placed ontop of the bottom layer\"\"\"\n",
    "    top_model = bottom_model.output\n",
    "    top_model = Flatten(name=\"flatten\")(top_model)\n",
    "    top_model = Dense(D,activation='relu')(top_model) \n",
    "    top_model = Dropout(0.3)(top_model)\n",
    "    top_model = Dense(num_classes,activation='softmax')(top_model)\n",
    "    return top_model\n",
    "\n",
    "#Note this different way of creating model. It has all the layers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84a7fc6-63d2-4d05-a34a-4c5fa3b977b6",
   "metadata": {},
   "source": [
    "## Let's add our FC Head back onto VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d235fbe9-0643-41dd-9970-595a431e784b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 17)                4369      \n",
      "=================================================================\n",
      "Total params: 21,141,841\n",
      "Trainable params: 6,427,153\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten,Conv2D,MaxPooling2D,ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "num_classes = 17 # 17 types of flowers\n",
    "\n",
    "FC_HEAD = addTopModel(vgg16,num_classes)\n",
    "\n",
    "model = Model(inputs=vgg16.input,outputs=FC_HEAD)\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ee5372b1-f373-4328-bc1b-2d2c9ef53b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_data_dir = './flowers/train'\n",
    "validation_data_dir = './flowers/validation'\n",
    "\n",
    "#performing data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "rescale = 1./255, #normalising\n",
    "rotation_range = 20,\n",
    "width_shift_range = 0.2,\n",
    "height_shift_range = 0.2,\n",
    "horizontal_flip=True,\n",
    "fill_mode = 'nearest')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b697ab6b-d502-4320-8da1-24de435e1823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1190 images belonging to 17 classes.\n",
      "Found 170 images belonging to 17 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batchsize = 32\n",
    "val_batchsize = 32\n",
    "\n",
    "#Configuring \n",
    "train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                   target_size = (img_rows,img_cols),\n",
    "                                                    batch_size = train_batchsize,\n",
    "                                                   class_mode = 'categorical')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(validation_data_dir,\n",
    "                                                             target_size = (img_rows,img_cols),\n",
    "                                                             batch_size = val_batchsize,\n",
    "                                                             class_mode = 'categorical',\n",
    "                                                             shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4f5aafba-5527-45a9-a785-427d3acd9655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab9df3b9-d3fc-4b45-901d-8da951a1d4be",
   "metadata": {},
   "source": [
    "## Training our top layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b5fbfbab-5af7-4c43-8e52-d0bf6b8625d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "38/74 [==============>...............] - ETA: 20s - loss: 2.3117 - accuracy: 0.3726WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1850 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n",
      "74/74 [==============================] - 25s 324ms/step - loss: 2.1424 - accuracy: 0.3921 - val_loss: 1.0243 - val_accuracy: 0.7588\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02433, saving model to .\\FlowerClassifier.h5\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./FlowerClassifier.h5\",\n",
    "                            monitor='val_loss',\n",
    "                            mode = 'min',\n",
    "                            save_best_only = True,\n",
    "                            verbose = 1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "#WE put our call back into call back list\n",
    "callbacks = [earlystop,checkpoint]\n",
    "\n",
    "#compiling the model\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = RMSprop(lr = 0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "nb_train_samples = len(train_generator)\n",
    "nb_validation_samples = 170\n",
    "epochs = 25\n",
    "batch_size = 16\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples//batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples//batch_size)\n",
    "\n",
    "model.save(\"./FlowerClassifier_savedModel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3b582202-c5b8-4e47-9fc3-5ee724cd9e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: pathspec 'FlowerClassifier(VGG+imgnet).ipynb' did not match any files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t../../.virtual_documents/\n",
      "\t../../Batch_Normalization_LeNet_Alexnet/.ipynb_checkpoints/\n",
      "\t../../Batch_Normalization_LeNet_Alexnet/Fashion_classifier.h5\n",
      "\t../../Batch_Normalization_LeNet_Alexnet/fashion_mnist/\n",
      "\t../../CallBacks+Fruit_classifier/.ipynb_checkpoints/\n",
      "\t../../CallBacks+Fruit_classifier/Fruits-360/\n",
      "\t../../CallBacks+Fruit_classifier/Untitled.ipynb\n",
      "\t../../CallBacks+Fruit_classifier/fruit_classifier.pickle1\n",
      "\t../../CallBacks+Fruit_classifier/fruits_fresh_cnn1.h5\n",
      "\t../../Confusion_Matrix/.ipynb_checkpoints/\n",
      "\t../../Data_Augmentation(DogxCat)/.ipynb_checkpoints/\n",
      "\t../../Data_Augmentation(DogxCat)/.virtual_documents/\n",
      "\t../../Data_Augmentation(DogxCat)/cats_vs_dogs_test_data.npz\n",
      "\t../../Data_Augmentation(DogxCat)/cats_vs_dogs_test_labels.npz\n",
      "\t../../Data_Augmentation(DogxCat)/cats_vs_dogs_training_data.npz\n",
      "\t../../Data_Augmentation(DogxCat)/cats_vs_dogs_training_labels.npz\n",
      "\t../../Data_Augmentation(DogxCat)/datasets/\n",
      "\t../.ipynb_checkpoints/\n",
      "\t./\n",
      "\t../Monkey_classifier/.ipynb_checkpoints/\n",
      "\t../Monkey_classifier/monkey_breed/\n",
      "\t../../ImageNet(ResNet,VGG,INCEPTIONV3)/.ipynb_checkpoints/\n",
      "\t../../ImageNet(ResNet,VGG,INCEPTIONV3)/images/\n",
      "\t../../kite_tutorial-checkpoint.ipynb\n",
      "\t../../kite_tutorial.ipynb\n",
      "\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "# ! git filter-branch --tree-filter 'rm -rf FlowerClassifier.h5 ' HEAD\n",
    "# ! git stash save\n",
    "# ! git push origin main --force\n",
    "! git add FlowerClassifier(VGG+imgnet).ipynb\n",
    "! git commit -m \"13-05-2021/23:30\"\n",
    "# ! git pull origin main\n",
    "! git push -f origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4e65f423-af9b-4f3c-a6ec-8589ffd7c9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: git-filter-branch has a glut of gotchas generating mangled history\n",
      "\t rewrites.  Hit Ctrl-C before proceeding to abort, then use an\n",
      "\t alternative filtering tool such as 'git filter-repo'\n",
      "\t (https://github.com/newren/git-filter-repo/) instead.  See the\n",
      "\t filter-branch manual page for more details; to squelch this warning,\n",
      "\t set FILTER_BRANCH_SQUELCH_WARNING=1.\n",
      "Proceeding with filter-branch...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: Cannot open 'FineTuning_TransferLearning(Monkey+flower_classifier)/Flower_Classifier/nul': No such file or directory\n",
      "You need to run this command from the toplevel of the working tree.\n"
     ]
    }
   ],
   "source": [
    "!git filter-branch -f --index-filter 'git rm --cached --ignore-unmatch fixtures/11_user_answer.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc17951-9b94-4e99-9a0c-bf2c694e1698",
   "metadata": {},
   "outputs": [],
   "source": [
    "FlowerClassifier_savedModel.h5 FlowerClassifier.h5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cv]",
   "language": "python",
   "name": "conda-env-cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

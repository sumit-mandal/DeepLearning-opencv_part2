{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LittleVGG for EMotion Detection\n",
    "\n",
    "### Training Emotion Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten,BatchNormalization\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28273 images belonging to 6 classes.\n",
      "Found 3534 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "num_classes = 6\n",
    "img_rows,img_cols = 48,48\n",
    "batch_size = 32\n",
    "\n",
    "train_data_dir = './fer2013/train'\n",
    "validation_data_dir = './fer2013/validation'\n",
    "\n",
    "#let's do some data augmentation\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                  rotation_range = 30,\n",
    "                                  shear_range = 0.3,\n",
    "                                  zoom_range = 0.3,\n",
    "                                  width_shift_range=0.4,\n",
    "                                  height_shift_range=0.4)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator  = train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                    color_mode = 'grayscale',\n",
    "                                                    target_size =(img_rows,img_cols),\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    class_mode = 'categorical',\n",
    "                                                    shuffle = True)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(validation_data_dir,\n",
    "                                                             color_mode = 'grayscale',\n",
    "                                                             target_size = (img_rows,img_cols),\n",
    "                                                             batch_size = batch_size,\n",
    "                                                             class_mode = 'categorical',\n",
    "                                                             shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 390       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 1,328,102\n",
      "Trainable params: 1,325,926\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",\n",
    "                 input_shape = (img_rows, img_cols, 1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), padding = \"same\", kernel_initializer=\"he_normal\", \n",
    "                 input_shape = (img_rows, img_cols, 1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #2: second CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #3: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #4: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #5: first set of FC => RELU layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #6: second set of FC => RELU layers\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #7: softmax classifier\n",
    "model.add(Dense(num_classes, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-d0af13bb0a24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m history = model.fit_generator(train_generator,\n\u001b[0m\u001b[0;32m     35\u001b[0m                              \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_train_samples\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                              \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_generator' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop,SGD,Adam\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./Emotion_detection.h5\",\n",
    "                            monitor='val_loss',\n",
    "                            mode = 'min',\n",
    "                            save_best_only = True,\n",
    "                            verbose = 1)\n",
    "\n",
    "earlystop= EarlyStopping(monitor='val_loss',\n",
    "                        min_delta = 0,\n",
    "                        patience = 5,\n",
    "                        verbose = 1,\n",
    "                        restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                             factor = 0.2,\n",
    "                             patience = 5,\n",
    "                             verbose = 1,\n",
    "                             min_delta=0.0001)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]\n",
    "\n",
    "# Compiling\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=Adam(lr=0.001),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "nb_train_samples = 28273\n",
    "nb_validation_samples =3534\n",
    "epochs = 50\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                             steps_per_epoch=nb_train_samples//batch_size,\n",
    "                             epochs = epochs,\n",
    "                             callbacks = callbacks,\n",
    "                             validation_data = validation_generator,\n",
    "                             validation_steps=nb_validation_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3534 images belonging to 6 classes.\n",
      "Class labels are {0: 'Angry', 1: 'Fear', 2: 'Happy', 3: 'Neutral', 4: 'Sad', 5: 'Surprise'}\n",
      "Classes are ['Angry', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import  matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "nb_train_samples = 28273  \n",
    "nb_validation_samples = 3534\n",
    "\n",
    "# we need to recreate our validation generator with shuffle = false\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "validation_data_dir,\n",
    "color_mode='grayscale',\n",
    "target_size = (img_rows,img_cols),\n",
    "batch_size = batch_size,\n",
    "class_mode='categorical',\n",
    "shuffle = False)\n",
    "\n",
    "class_labels = validation_generator.class_indices # it gives the classes labels\n",
    "# {'Angry': 0, 'Fear': 1, 'Happy': 2, 'Neutral': 3, 'Sad': 4, 'Surprise': 5}\n",
    "class_labels = {v:k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "print(\"Class labels are\",class_labels)\n",
    "\n",
    "print(\"Classes are\",classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumit\\anaconda3\\envs\\cv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3534, 6)\n",
      "[[2.16823071e-01 1.06542654e-01 1.55494325e-02 3.70724261e-01\n",
      "  2.79020876e-01 1.13396244e-02]\n",
      " [9.12040293e-01 7.03974888e-02 5.12215542e-03 7.83300784e-04\n",
      "  5.53372130e-03 6.12307200e-03]\n",
      " [6.51892781e-01 1.50442481e-01 2.82125007e-02 3.24218012e-02\n",
      "  1.25886247e-01 1.11441873e-02]\n",
      " ...\n",
      " [1.32296272e-02 1.24650128e-01 7.54520996e-03 5.09491377e-03\n",
      "  3.56036797e-03 8.45919728e-01]\n",
      " [3.32851522e-02 1.67721659e-01 1.06800431e-02 2.41453007e-01\n",
      "  7.81687945e-02 4.68691349e-01]\n",
      " [6.24770746e-02 3.07334542e-01 3.19395550e-02 1.39449500e-02\n",
      "  2.28791535e-02 5.61424673e-01]]\n",
      "(3534,)\n",
      "[3 0 0 ... 5 5 5]\n",
      "Confusion Matrix\n",
      "[[229  29  27  71 123  12]\n",
      " [ 90  89  30  65 175  79]\n",
      " [ 11  14 767  29  39  19]\n",
      " [ 95  40 153 121 155  62]\n",
      " [ 35  20  35 134 359  11]\n",
      " [ 15  37  30  14  13 307]]\n",
      "Classification report\n",
      "target names are: ['Angry', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.48      0.47      0.47       491\n",
      "        Fear       0.39      0.17      0.24       528\n",
      "       Happy       0.74      0.87      0.80       879\n",
      "     Neutral       0.28      0.19      0.23       626\n",
      "         Sad       0.42      0.60      0.49       594\n",
      "    Surprise       0.63      0.74      0.68       416\n",
      "\n",
      "    accuracy                           0.53      3534\n",
      "   macro avg       0.49      0.51      0.48      3534\n",
      "weighted avg       0.50      0.53      0.50      3534\n",
      "\n",
      "Tickmarkks: [0 1 2 3 4 5]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Cnfusion matrix and classification report\n",
    "Y_pred = model.predict_generator(validation_generator,nb_validation_samples // batch_size+1)\n",
    "print(Y_pred.shape)\n",
    "print(Y_pred)\n",
    "\n",
    "y_pred = np.argmax(Y_pred,axis=1) #np.argmax Returns the indices of the maximum values along an axis.\n",
    "print(y_pred.shape)\n",
    "print(y_pred)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(validation_generator.classes,y_pred))\n",
    "print('Classification report')\n",
    "target_names = list(class_labels.values())\n",
    "print(\"target names are:\",target_names)\n",
    "print(classification_report(validation_generator.classes,y_pred,target_names=target_names))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "cnf_matrix = confusion_matrix(validation_generator.classes,y_pred)\n",
    "\n",
    "plt.imshow(cnf_matrix,interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "print(\"Tickmarkks:\",tick_marks)\n",
    "\n",
    "_ = plt.xticks(tick_marks,classes,rotation=90)\n",
    "_ = plt.yticks(tick_marks,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(\"./Emotion_detection.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on validation images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "def draw_test(name, pred, im, true_label):\n",
    "    BLACK = [0,0,0]\n",
    "    expanded_image = cv2.copyMakeBorder(im, 150, 0, 0, 300 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    cv2.putText(expanded_image, \"predicted - \"+ pred, (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
    "    cv2.putText(expanded_image, \"true - \"+ true_label, (20, 120) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "    cv2.imshow(name, expanded_image)\n",
    "\n",
    "\n",
    "def getRandomImage(path, img_width, img_height):\n",
    "    \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
    "    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n",
    "    random_directory = np.random.randint(0,len(folders))\n",
    "    path_class = folders[random_directory]\n",
    "    file_path = path + path_class\n",
    "    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "    random_file_index = np.random.randint(0,len(file_names))\n",
    "    image_name = file_names[random_file_index]\n",
    "    final_path = file_path + \"/\" + image_name\n",
    "    return image.load_img(final_path, target_size = (img_width, img_height),grayscale=True), final_path, path_class\n",
    "\n",
    "# dimensions of our images\n",
    "img_width, img_height = 48, 48\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = RMSprop(lr = 0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "files = []\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# predicting images\n",
    "for i in range(0, 10):\n",
    "    path = './fer2013/validation/' \n",
    "    img, final_path, true_label = getRandomImage(path, img_width, img_height)\n",
    "    files.append(final_path)\n",
    "    true_labels.append(true_label)\n",
    "    x = image.img_to_array(img)\n",
    "    x = x * 1./255\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    images = np.vstack([x])\n",
    "    classes = model.predict_classes(images, batch_size = 10)\n",
    "    predictions.append(classes)\n",
    "    \n",
    "for i in range(0, len(files)):\n",
    "    image = cv2.imread((files[i]))\n",
    "    image = cv2.resize(image, None, fx=3, fy=3, interpolation = cv2.INTER_CUBIC)\n",
    "    draw_test(\"Prediction\", class_labels[predictions[i][0]], image, true_labels[i])\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEst on single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:19: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:19: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-71-ce6a8148a990>:19: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if faces is ():\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('./Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img.copy(),cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "    allfaces = []   \n",
    "    rects = []\n",
    "    for (x,y,w,h) in faces:  #x,y,w,h are bounding rectangles around face\n",
    "    #x,y are the top left axes and w and h are width and hheight respectivel\"\"\"\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "        allfaces.append(roi_gray)\n",
    "        rects.append((x,w,y,h))\n",
    "    return rects, allfaces, img\n",
    "\n",
    "img = cv2.imread(\"sumit.jpg\")\n",
    "rects, faces, image = face_detector(img)\n",
    "\n",
    "i = 0\n",
    "for face in faces:\n",
    "    roi = face.astype(\"float\") / 255.0\n",
    "    roi = img_to_array(roi)\n",
    "    roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "    # make a prediction on the ROI, then lookup the class\n",
    "    preds = model.predict(roi)[0]\n",
    "    label = class_labels[preds.argmax()]   \n",
    "\n",
    "    #Overlay our detected emotion on our pic\n",
    "    label_position = (rects[i][0] + int((rects[i][1]/2)), abs(rects[i][2] - 10))\n",
    "    i =+ 1\n",
    "    cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "    \n",
    "cv2.imshow(\"Emotion Detector\", image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making this on webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:12: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-73-9c87e821f8d6>:12: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if faces is ():\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('./Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "\n",
    "    try:\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "    except:\n",
    "        return (x,w,y,h), np.zeros((48,48), np.uint8), img\n",
    "    return (x,w,y,h), roi_gray, img\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    rect, face, image = face_detector(frame)\n",
    "    if np.sum([face]) != 0.0:\n",
    "        roi = face.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "        # make a prediction on the ROI, then lookup the class\n",
    "        preds = model.predict(roi)[0]\n",
    "        label = class_labels[preds.argmax()]  \n",
    "        label_position = (rect[0] + int((rect[1]/2)), rect[2] + 25)\n",
    "        cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "    else:\n",
    "        cv2.putText(image, \"No Face Found\", (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "        \n",
    "    cv2.imshow('All', image)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git add Emotion_detection.h5 Emotion_detection.ipynb\n",
    "! git commit -m \"15:56/15-05-2021\"\n",
    "! git push origin main"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cv] *",
   "language": "python",
   "name": "conda-env-cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
